# 1. Input: Kafka (Using JSON codec)
input {
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["log-topic"]             
    codec => "json" 
    auto_offset_reset => "earliest"
  }
}

# 2. Filter: Grok and Date Parsing
filter {
    # 1. Grok the raw line provided by the producer
    grok {
        # Format: TIME (16:31:01), LOGLEVEL (INFO), MESSAGE
        match => { "raw_line" => "^%{TIME:log_time}\s+%{LOGLEVEL:log_level}\s+-\s+%{GREEDYDATA:event_message}" }
        add_tag => ["build_log"]
        tag_on_failure => ["unstructured_meta_line"]
    }
    
    # 2. Date Filter (Construct the final @timestamp)
    if [folder_date] and [log_time] {
        # Combine the date from the producer and the time from Grok
        mutate {
            add_field => { "temp_timestamp" => "%{folder_date} %{log_time}" }
        }
        
        # Parse the combined field to set the Elasticsearch @timestamp
        date {
            match => ["temp_timestamp", "YYYY-MM-dd HH:mm:ss"]
            target => "@timestamp"
        }
        
        # 3. Cleanup: Remove the temporary and redundant fields
        mutate {
            remove_field => ["log_time", "temp_timestamp", "raw_line"]
        }
    }
    
    # 4. Handle lines that failed the main Grok pattern (e.g., '========= Started...')
    if "unstructured_meta_line" in [tags] {
        mutate {
            add_field => { "log_level" => "META" }
            # Rename the original message to fit the field structure
            rename => { "raw_line" => "event_message" }
            remove_field => ["log_time"] # Remove log_time if present from a partial match
        }
    } else {
        # Keep the cleanup of the raw_line field for successfully grokked lines
        mutate {
            remove_field => ["raw_line"]
        }
    }
}

# 3. Output: Elasticsearch
output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "mozilla-log-%{+YYYY.MM.dd}"
    # stdout { codec => rubydebug }
  }
}